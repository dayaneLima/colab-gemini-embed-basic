{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOaCvfLfsl1j2VUYpDo4G/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dayaneLima/colab-gemini-embed-basic/blob/main/ImersaoAluraEmbed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando o SDK do Google Generative AI"
      ],
      "metadata": {
        "id": "F_p2Kq-VE1in"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EKz338uoECKm"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar as libs"
      ],
      "metadata": {
        "id": "If_FXLSIFul1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suporte para arrays e matrizes multidimensionais, junto com uma grande coleção de funções matemáticas de alto nível\n",
        "# Bem a grosso modo: ferramenta matemática\n",
        "# Não foi preciso instalar pq já vem imputida no Google Colab\n",
        "import numpy as np\n",
        "\n",
        "# Análise e manipulação de dados fornecendo estruturas de dados de alto desempenho e funções poderosas\n",
        "# Bem a grosso modo: espécie de google sheets\n",
        "# Não foi preciso instalar pq já vem imputida no Google Colab\n",
        "import pandas as pd\n",
        "\n",
        "# Import the Python SDK\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "ER8jzcZ5F1io"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurar API Key\n",
        "\n",
        "Para mais informações: [Get an API key](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Faistudio.google.com%2Fapp%2Fapikey)"
      ],
      "metadata": {
        "id": "04BK7LLvSdgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "PcZPtvbrSeBO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listar os modelos disponíveis"
      ],
      "metadata": {
        "id": "9gtNLIpUInAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'embedContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oKmoLd3qInp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed: O que é?\n",
        "\n",
        "No Gemini, um embed pode ser entendido como uma representação vetorial do significado de um texto. Essa representação, na forma de uma matriz multidimensional, é criada por meio de um processo chamado \"embedding\".\n",
        "\n",
        "Imagine um enorme mapa onde cada palavra e frase possui um ponto específico. A posição desse ponto no mapa, representada por suas coordenadas em diversas dimensões, captura a essência do significado da palavra ou frase. Quanto mais próximos os pontos estiverem no mapa, mais similar será o significado das palavras ou frases que eles representam.\n",
        "\n",
        "O processo de embedding funciona da seguinte maneira:\n",
        "\n",
        "1. Entrada: O texto a ser representado é fornecido como entrada para o modelo de embedding.\n",
        "2. Processamento: O modelo de embedding analisa o texto e identifica padrões e relações entre as palavras.\n",
        "3. Saída: O modelo de embedding gera uma matriz multidimensional que representa o significado do texto.\n",
        "\n",
        "Essas representações vetoriais podem ser usadas para diversas tarefas de processamento de linguagem natural (PLN), como:\n",
        "\n",
        "- Pesquisa semântica: Encontrar documentos que sejam semanticamente semelhantes a um documento de consulta, mesmo que usem palavras diferentes.\n",
        "- Classificação de texto: Classificar documentos em categorias predefinidas, como \"notícia\", \"artigo científico\" ou \"receita\".\n",
        "- Agrupamento de texto: Agrupar documentos com base em tópicos semelhantes, identificando automaticamente temas recorrentes.\n",
        "\n",
        "O Gemini oferece diversos modelos de embedding de texto, cada um com suas próprias características e vantagens. A escolha do modelo ideal depende da tarefa específica que você está realizando."
      ],
      "metadata": {
        "id": "ikvBBxUvSN-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed content"
      ],
      "metadata": {
        "id": "mAg1uuGrSEQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de tamanho de vetor gerado\n",
        "\n",
        "\n",
        "Chamada do método embed_content com o modelo models/text-embedding-004 para gerar embeddings de texto."
      ],
      "metadata": {
        "id": "YdBTxwTafoZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world\"\n",
        "result = genai.embed_content(model=\"models/text-embedding-004\", content=text)\n",
        "\n",
        "# Print just a part of the embedding to keep the output manageable\n",
        "print(str(result['embedding'])[:50], '... TRIMMED]')\n",
        "\n",
        "# Print\n",
        "print(result['embedding'])\n"
      ],
      "metadata": {
        "id": "cv3wMXzqSW5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(result['embedding'])) # número de dimensões"
      ],
      "metadata": {
        "id": "mEnBJwSZU4HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste com outro texto.\n",
        "\n",
        "Irá gerar uma outra matriz, com valores diferentes, mas com o mesmo tamanho, para facilitar a comparação.\n"
      ],
      "metadata": {
        "id": "0-Ba3k7gfEYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Teste de embedding com outro texto\"\n",
        "result2 = genai.embed_content(model=\"models/text-embedding-004\", content=text)\n",
        "\n",
        "# Print just a part of the embedding to keep the output manageable\n",
        "print(str(result2['embedding'])[:50], '... TRIMMED]')\n",
        "\n",
        "# Print\n",
        "print(result2['embedding'])\n"
      ],
      "metadata": {
        "id": "qW5y7tJZfHy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(result2['embedding'])) # número de dimensões"
      ],
      "metadata": {
        "id": "7qIjklCjfLvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch embed content\n",
        "\n",
        "Você pode incorporar uma lista de vários prompts com uma chamada de API para maior eficiência."
      ],
      "metadata": {
        "id": "W8_P0Cn9cHcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=[\n",
        "      'What is the meaning of life?',\n",
        "      'How much wood would a woodchuck chuck?',\n",
        "      'How does the brain work?'])\n",
        "\n",
        "for embedding in result['embedding']:\n",
        "  print(len(embedding))"
      ],
      "metadata": {
        "id": "7y2hpYiRcJ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Embedding\n",
        "\n",
        "O task_type escolhido vai ser de acordo com o que deseja com o texto que será gerado o embedding.\n",
        "\n",
        "\n",
        "* TASK_TYPE_UNSPECIFIED:\tUnset value, which will default to one of the other enum values.\n",
        "* RETRIEVAL_QUERY:\tSpecifies the given text is a query in a search/retrieval setting.\n",
        "* RETRIEVAL_DOCUMENT:\tSpecifies the given text is a document from the corpus being searched.\n",
        "* SEMANTIC_SIMILARITY:\tSpecifies the given text will be used for STS.\n",
        "* CLASSIFICATION:\tSpecifies that the given text will be classified.\n",
        "* CLUSTERING:\tSpecifies that the embeddings will be used for clustering.\n",
        "* QUESTION_ANSWERING:\tSpecifies that the given text will be used for question answering.\n",
        "* FACT_VERIFICATION:\tSpecifies that the given text will be used for fact verification."
      ],
      "metadata": {
        "id": "EN4mem3gdKiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"A próxima geração de IA para desenvolvedores e Google Workspace\"\n",
        "sample_text = (\"Título: A próxima geração de IA para desenvolvedores e Google Workspace\"\n",
        "    \"\\n\"\n",
        "    \"Artigo completo:\\n\"\n",
        "    \"\\n\"\n",
        "    \"Gemini API & Google AI Studio: Uma maneira acessível de explorar e criar protótipos com aplicações de IA generativa\")\n",
        "\n",
        "embeddings = genai.embed_content(model=\"models/embedding-001\",\n",
        "                                 content=sample_text,\n",
        "                                 title=title,\n",
        "                                 task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "_gmI9LICdPVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação de documentos que serão buscados"
      ],
      "metadata": {
        "id": "fJyKax5LiqFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Listagem de documentos que serão buscados\n",
        "DOCUMENT1 = {\n",
        "    \"Título\": \"Operação do sistema de controle climático\",\n",
        "    \"Conteúdo\": \"O Googlecar tem um sistema de controle climático que permite ajustar a temperatura e o fluxo de ar no carro. Para operar o sistema de controle climático, use os botões e botões localizados no console central.  Temperatura: O botão de temperatura controla a temperatura dentro do carro. Gire o botão no sentido horário para aumentar a temperatura ou no sentido anti-horário para diminuir a temperatura. Fluxo de ar: O botão de fluxo de ar controla a quantidade de fluxo de ar dentro do carro. Gire o botão no sentido horário para aumentar o fluxo de ar ou no sentido anti-horário para diminuir o fluxo de ar. Velocidade do ventilador: O botão de velocidade do ventilador controla a velocidade do ventilador. Gire o botão no sentido horário para aumentar a velocidade do ventilador ou no sentido anti-horário para diminuir a velocidade do ventilador. Modo: O botão de modo permite que você selecione o modo desejado. Os modos disponíveis são: Auto: O carro ajustará automaticamente a temperatura e o fluxo de ar para manter um nível confortável. Cool (Frio): O carro soprará ar frio para dentro do carro. Heat: O carro soprará ar quente para dentro do carro. Defrost (Descongelamento): O carro soprará ar quente no para-brisa para descongelá-lo.\"}\n",
        "\n",
        "DOCUMENT2 = {\n",
        "    \"Título\": \"Touchscreen\",\n",
        "    \"Conteúdo\": \"O seu Googlecar tem uma grande tela sensível ao toque que fornece acesso a uma variedade de recursos, incluindo navegação, entretenimento e controle climático. Para usar a tela sensível ao toque, basta tocar no ícone desejado.  Por exemplo, você pode tocar no ícone \\\"Navigation\\\" (Navegação) para obter direções para o seu destino ou tocar no ícone \\\"Music\\\" (Música) para reproduzir suas músicas favoritas.\"}\n",
        "\n",
        "DOCUMENT3 = {\n",
        "    \"Título\": \"Mudança de marchas\",\n",
        "    \"Conteúdo\": \"Seu Googlecar tem uma transmissão automática. Para trocar as marchas, basta mover a alavanca de câmbio para a posição desejada.  Park (Estacionar): Essa posição é usada quando você está estacionado. As rodas são travadas e o carro não pode se mover. Marcha à ré: Essa posição é usada para dar ré. Neutro: Essa posição é usada quando você está parado em um semáforo ou no trânsito. O carro não está em marcha e não se moverá a menos que você pressione o pedal do acelerador. Drive (Dirigir): Essa posição é usada para dirigir para frente. Low: essa posição é usada para dirigir na neve ou em outras condições escorregadias.\"}\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "vUaZ7SC1irGK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando a lib pandas para formatar os documentos em formato de tabela"
      ],
      "metadata": {
        "id": "h1kkwcUAjEkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(documents)\n",
        "df.columns = [\"Titulo\", \"Conteudo\"]\n",
        "df"
      ],
      "metadata": {
        "id": "zLv_l-ObjFDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação de uma função para automatizar a criação dos embeddings"
      ],
      "metadata": {
        "id": "dEvZ9N0gkj3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escolha do modelo:"
      ],
      "metadata": {
        "id": "NBQl6exLkpfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"models/embedding-001\""
      ],
      "metadata": {
        "id": "MkStFNoTjKhH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação da função em python para gerar o embedding e o retornar"
      ],
      "metadata": {
        "id": "5heo6D76kwze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_fn(title, text):\n",
        "  return genai.embed_content(model=model,\n",
        "                                 content=text,\n",
        "                                 title=title,\n",
        "                                 task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]"
      ],
      "metadata": {
        "id": "TFeVSM9AjN7P"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada item no dataframe que foi gerado dos documentos, será chamado a função embed_fn criada, informando o Titulo e o Conteudo e armazenando em uma nova coluna desse dataframe(df), chamada Embeddings.\n",
        "\n",
        "O comando lambda é para informar que será percorrido e armazenado linha por linha."
      ],
      "metadata": {
        "id": "L0in2QHHlFQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Embeddings\"] = df.apply(lambda row: embed_fn(row[\"Titulo\"], row[\"Conteudo\"]), axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "QcqAIxzxjVpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação de uma função para realização de consultas nos documentos\n",
        "\n",
        "Para gerar uma consulta, é necessário criar o embedding da mesma, para poder comparar com os documentos no momento da busca."
      ],
      "metadata": {
        "id": "Cd1UImf_l3mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_e_buscar_consulta(consulta, base, model):\n",
        "\n",
        "  # Geracao do embedding do texto da consulta\n",
        "  # task_type de RETRIEVAL_QUERY por ser para consulta\n",
        "  embedding_da_consulta = genai.embed_content(model=model,\n",
        "                                 content=consulta,\n",
        "                                 task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
        "\n",
        "  # Calcular a distância entre os vetores da consulta e dos documentos\n",
        "  # É utilizado os embeddings dos documentos gerados (df[\"Embeddings\"])\n",
        "  # e o embedding da consulta\n",
        "  # Utilizado a lib numpy para manipulação de array para realizar a comparação\n",
        "  # calculando então a distancia entre os dataframes e a consulta,\n",
        "  # para assim encontar a mais próxima\n",
        "  produtos_escalares = np.dot(np.stack(df[\"Embeddings\"]), embedding_da_consulta)\n",
        "\n",
        "  # obter do np o índice do maior (o mais próximo da consulta)\n",
        "  indice = np.argmax(produtos_escalares)\n",
        "\n",
        "  # retorna do dataframe o elemento de índice maior encontrado\n",
        "  return df.iloc[indice][\"Conteudo\"]"
      ],
      "metadata": {
        "id": "7rKjOXe-jYEI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação do texto da consulta e chamando a função criada."
      ],
      "metadata": {
        "id": "juZ-IGvrokeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consulta = \"Como faço para trocar marchas em um carro do Google?\"\n",
        "\n",
        "trecho = gerar_e_buscar_consulta(consulta, df, model)\n",
        "print(trecho)"
      ],
      "metadata": {
        "id": "XzpB_RfojZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizar o trecho encontrado e solicitar para encrevê-lo de forma mais descontraída\n",
        "\n",
        "Abaixo é adicionado a configuração para o GenerativeModel:"
      ],
      "metadata": {
        "id": "fkaWEtmyosHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "  \"temperature\": 0,\n",
        "  \"candidate_count\": 1\n",
        "}"
      ],
      "metadata": {
        "id": "hCKEgleEjcfo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação do prompt da para encrever o trecho obtido de forma mais descontraída e chamando o GenerativeModel juntamente do generate_content informando o prompt."
      ],
      "metadata": {
        "id": "hYo6gU9QosqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"Reescreva esse texto de uma forma mais descontraída, sem adicionar informações que não façam parte do texto: {trecho}\"\n",
        "\n",
        "model_2 = genai.GenerativeModel(\"gemini-1.0-pro\",\n",
        "                                generation_config=generation_config)\n",
        "response = model_2.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "cmWshrjwjd01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}